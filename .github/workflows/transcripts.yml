name: Pull YouTube transcripts (multi-client with API fallback)

on:
  workflow_dispatch:
  schedule:
    - cron: "17 3 * * *"  # daily at 03:17 UTC

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install yt-dlp tqdm requests youtube-transcript-api==0.6.2

      - name: Restore cookies.txt from secret (optional)
        run: |
          if [ -n "${{ secrets.YT_COOKIES_B64 }}" ]; then
            echo "${{ secrets.YT_COOKIES_B64 }}" | base64 -d > cookies.txt
            echo "cookies.txt restored from secret"
            head -n 2 cookies.txt || true
          else
            echo "No YT_COOKIES_B64 secret set; yt-dlp may hit bot checks."
          fi

      - name: Download subtitles (Android then Web client)
        env:
          PLAYLIST_URL: "https://www.youtube.com/playlist?list=PLstjectj9BFgWGjHn4y2oygN34oFpSPjR"
        run: |
          mkdir -p transcripts
          run_dl () {
            local CLIENT="$1"
            echo "==> Trying player_client=${CLIENT}"
            yt-dlp \
              ${YT_COOKIES_B64:+--cookies cookies.txt} \
              --ignore-errors \
              --yes-playlist \
              --skip-download \
              --write-sub --write-auto-sub \
              --sub-langs "all" \
              --sub-format vtt \
              --extractor-args "youtube:player_client=${CLIENT}" \
              --force-ipv4 \
              --sleep-requests 1 \
              --sleep-interval 0.5 \
              --max-sleep-interval 1.5 \
              --retries 10 \
              --output "transcripts/%(title)s [%(id)s].%(ext)s" \
              "$PLAYLIST_URL" || true
          }
          run_dl android
          run_dl web

      - name: Convert VTT to TXT and index any language
        run: |
          python - <<'PY'
          import csv, glob, os, re
          from pathlib import Path

          def vtt_to_txt(p):
              out = []
              with open(p, 'r', encoding='utf-8', errors='ignore') as f:
                  for line in f:
                      s = line.rstrip('\n')
                      if s.startswith('WEBVTT') or s.strip().startswith(('NOTE','STYLE')):
                          continue
                      if re.match(r'^\d{2}:\d{2}:\d{2}\.\d{3}\s+-->\s+\d{2}:\d{2}:\d{2}\.\d{3}', s):
                          continue
                      if not s.strip() or re.match(r'^\s*\d+\s*$', s):
                          continue
                      out.append(s)
              txt = '\n'.join(out).strip()
              txt = re.sub(r'\n{3,}', '\n\n', txt)
              return txt

          Path('transcripts').mkdir(exist_ok=True)
          rows = []
          for vtt in glob.glob('transcripts/*.vtt'):
              base = os.path.basename(vtt)
              m = re.match(r'^(?P<title>.+) \[(?P<id>[-\w]{6,})\](?:\.(?P<lang>[\w-]+))?\.vtt$', base)
              if not m:
                  continue
              title, vid, lang = m.group('title'), m.group('id'), m.group('lang') or 'unknown'
              url = f'https://www.youtube.com/watch?v={vid}'

              txt_path = f'transcripts/{title} [{vid}].txt'
              if not os.path.exists(txt_path):
                  with open(txt_path, 'w', encoding='utf-8') as f:
                      f.write(vtt_to_txt(vtt))

              char_len = os.path.getsize(txt_path) if os.path.exists(txt_path) else 0
              rows.append({
                  'video_id': vid,
                  'url': url,
                  'title': title,
                  'source': f'yt-dlp.vtt({lang})',
                  'char_len': char_len
              })

          # Write initial CSV (dedupe preferring English if present)
          best = {}
          pref = ['en', 'en-US', 'en-GB']
          for r in rows:
              vid = r['video_id']
              lang = r['source'].split('(')[-1].rstrip(')') if '(' in r['source'] else ''
              if vid not in best:
                  best[vid] = r
              else:
                  cur_lang = best[vid]['source'].split('(')[-1].rstrip(')') if '(' in best[vid]['source'] else ''
                  if cur_lang not in pref and lang in pref:
                      best[vid] = r
          rows = list(best.values())

          with open('transcripts.csv', 'w', newline='', encoding='utf-8') as f:
              w = csv.DictWriter(f, fieldnames=['video_id','url','title','source','char_len'])
              w.writeheader()
              w.writerows(sorted(rows, key=lambda r: (r['title'].lower(), r['video_id'])))
          PY

      - name: API fallback for missing videos
        env:
          PLAYLIST_ID: "PLstjectj9BFgWGjHn4y2oygN34oFpSPjR"
          YT_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python - <<'PY'
          import csv, os, re, sys, json, time
          from pathlib import Path
          from urllib.parse import urlencode
          import requests
          from tqdm import tqdm
          from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, VideoUnavailable

          PLAYLIST_ID = os.environ["PLAYLIST_ID"]
          API_KEY = os.environ.get("YT_API_KEY")
          out_dir = Path("transcripts")
          csv_path = Path("transcripts.csv")

          def safe_filename(name: str, suffix: str, max_len: int = 120) -> str:
              import unicodedata
              n = unicodedata.normalize("NFKC", name or "")
              n = "".join(ch for ch in n if ch.isprintable())
              n = re.sub(r'[\\/:*?"<>|]+', " ", n)
              n = re.sub(r"\s+", " ", n).strip()
              base_max = max(1, max_len - len(suffix))
              if len(n) > base_max:
                  n = n[:base_max].rstrip()
              return f"{n}{suffix}"

          def list_playlist_items(playlist_id: str, api_key: str):
              items, page_token = [], None
              base = "https://www.googleapis.com/youtube/v3/playlistItems"
              while True:
                  params = {
                      "part": "snippet",
                      "maxResults": 50,
                      "playlistId": playlist_id,
                      "key": api_key,
                  }
                  if page_token:
                      params["pageToken"] = page_token
                  url = f"{base}?{urlencode(params)}"
                  r = requests.get(url, timeout=30)
                  if r.status_code != 200:
                      break
                  data = r.json()
                  for e in data.get("items", []):
                      sn = e.get("snippet", {})
                      title = (sn.get("title") or "").strip()
                      vid = (sn.get("resourceId", {}) or {}).get("videoId")
                      if vid and title and title.lower() != "private video":
                          items.append({"video_id": vid, "title": title})
                  page_token = data.get("nextPageToken")
                  if not page_token:
                      break
                  time.sleep(0.2)
              return items

          def fetch_transcript_any_to_en(vid: str):
              try:
                  return YouTubeTranscriptApi.get_transcript(vid, languages=["en"]), "en"
              except Exception:
                  pass
              try:
                  listing = YouTubeTranscriptApi.list_transcripts(vid)
              except Exception:
                  raise NoTranscriptFound("list_transcripts failed")
              try:
                  t = listing.find_manually_created_transcript(["en"])
                  return t.fetch(), "en"
              except Exception:
                  try:
                      t = listing.find_generated_transcript(["en"])
                      return t.fetch(), "en-auto"
                  except Exception:
                      pass
              for tr in listing:
                  if getattr(tr, "is_translatable", False):
                      try:
                          en_t = tr.translate("en")
                          return en_t.fetch(), f"{tr.language_code}->en"
                      except Exception:
                          continue
              raise NoTranscriptFound("no captions")

          existing_ids = set()
          with open(csv_path, newline='', encoding='utf-8') as f:
              for row in csv.DictReader(f):
                  existing_ids.add(row['video_id'])

          if not API_KEY:
              print("No API key; skipping fallback.")
              sys.exit(0)

          items = list_playlist_items(PLAYLIST_ID, API_KEY)
          rows = []
          for it in tqdm(items, desc="API fallback"):
              vid, title = it["video_id"], it["title"]
              if vid in existing_ids:
                  continue
              url = f"https://www.youtube.com/watch?v={vid}"
              txt_name = safe_filename(title, f" [{vid}].txt")
              srt_name = safe_filename(title, f" [{vid}].srt")
              try:
                  chunks, src = fetch_transcript_any_to_en(vid)
                  text = " ".join((x.get("text") or "").replace("\n"," ").strip() for x in chunks if x.get("text"))
                  if text.strip():
                      (out_dir / txt_name).write_text(text, encoding="utf-8")
                      (out_dir / srt_name).write_text(text, encoding="utf-8")
                  rows.append({"video_id": vid, "url": url, "title": title, "source": src, "char_len": len(text)})
              except (TranscriptsDisabled, NoTranscriptFound):
                  rows.append({"video_id": vid, "url": url, "title": title, "source": "none", "char_len": 0})
              except VideoUnavailable:
                  rows.append({"video_id": vid, "url": url, "title": title, "source": "unavailable", "char_len": 0})
              except Exception as e:
                  rows.append({"video_id": vid, "url": url, "title": title, "source": f"error: {type(e).__name__}", "char_len": 0})

          # Append to CSV
          all_rows = []
          with open(csv_path, newline='', encoding='utf-8') as f:
              all_rows.extend(csv.DictReader(f))
          all_rows.extend(rows)
          with open(csv_path, 'w', newline='', encoding='utf-8') as f:
              w = csv.DictWriter(f, fieldnames=['video_id','url','title','source','char_len'])
              w.writeheader()
              w.writerows(sorted(all_rows, key=lambda r: (r['title'].lower(), r['video_id'])))
          PY

      - name: Commit results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          [ -e transcripts.csv ] && git add transcripts.csv || true
          [ -d transcripts ] && git add transcripts/* || true
          git status
          git commit -m "Update transcripts" || echo "No changes"
          git push
