name: Pull YouTube transcripts (10s pause, yt-dlp only)

on:
  workflow_dispatch:
  schedule:
    - cron: "17 3 * * *"  # daily at 03:17 UTC

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    env:
      PLAYLIST_URL: "https://www.youtube.com/playlist?list=PLstjectj9BFgWGjHn4y2oygN34oFpSPjR"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Set up Python + deps
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: |
          python -m pip install --upgrade pip
          pip install 'yt-dlp[default]'

      - name: Restore cookies.txt (optional but recommended)
        run: |
          if [ -n "${{ secrets.YT_COOKIES_B64 }}" ]; then
            printf "%s" "${{ secrets.YT_COOKIES_B64 }}" | base64 -d > cookies.txt
            echo "cookies.txt restored (head):"
            head -n 5 cookies.txt || true
            if ! grep -E "youtube\.com" cookies.txt >/dev/null 2>&1; then
              echo "WARNING: cookies.txt has no youtube.com entries"; fi
          else
            echo "No YT_COOKIES_B64 secret set; yt-dlp may be blocked."
          fi

      - name: Download English subtitles with cookies (tvhtml5 → web, 10s pause)
        run: |
          mkdir -p transcripts
          COOKIES_ARG=""
          [ -f cookies.txt ] && COOKIES_ARG="--cookies cookies.txt"

          run_dl () {
            local CLIENT="$1"
            echo "==> Trying player_client=${CLIENT}"
            yt-dlp \
              $COOKIES_ARG \
              --ignore-errors \
              --yes-playlist \
              --skip-download \
              --write-sub --write-auto-sub \
              --sub-langs "en,en-US,en-GB" \
              --sub-format vtt \
              --extractor-args "youtube:player_client=${CLIENT}" \
              --sleep-requests 3 \
              --sleep-interval 10 \
              --max-sleep-interval 10 \
              --retries 15 \
              --retries-subtitles 20 \
              --retry-sleep-subtitles 2:exp=1.8:max=90 \
              --limit-rate 80K \
              --throttled-rate 80K \
              --output "transcripts/%(title)s [%(id)s].%(ext)s" \
              "${PLAYLIST_URL}" || true
          }

          run_dl tvhtml5
          run_dl web

      - name: Convert VTT → TXT and build CSV
        run: |
          python - <<'PY'
          import csv, glob, os, re
          from pathlib import Path

          def vtt_to_txt(p):
              out=[]
              with open(p,'r',encoding='utf-8',errors='ignore') as f:
                  for line in f:
                      s=line.rstrip('\n')
                      if s.startswith('WEBVTT') or s.strip().startswith(('NOTE','STYLE')): continue
                      if re.match(r'^\d{2}:\d{2}:\d{2}\.\d{3}\s+-->\s+\d{2}:\d{2}:\d{2}\.\d{3}', s): continue
                      if not s.strip() or re.match(r'^\s*\d+\s*$', s): continue
                      out.append(s)
              import re as _re
              txt='\n'.join(out).strip()
              return _re.sub(r'\n{3,}','\n\n',txt)

          Path('transcripts').mkdir(exist_ok=True)
          entries=[]
          for vtt in glob.glob('transcripts/*.vtt'):
              base=os.path.basename(vtt)
              m=re.match(r'^(?P<title>.+) \[(?P<id>[-\w]{6,})\](?:\.(?P<lang>[\w-]+))?\.vtt$', base)
              if not m: continue
              title, vid, lang = m.group('title'), m.group('id'), m.group('lang') or 'unknown'
              url=f'https://www.youtube.com/watch?v={vid}'

              txt_path=f'transcripts/{title} [{vid}].txt'
              if not os.path.exists(txt_path):
                  with open(txt_path,'w',encoding='utf-8') as f:
                      f.write(vtt_to_txt(vtt))
              char_len=os.path.getsize(txt_path) if os.path.exists(txt_path) else 0
              entries.append({'video_id':vid,'url':url,'title':title,'source':f'yt-dlp.vtt({lang})','char_len':char_len})

          with open('transcripts.csv','w',newline='',encoding='utf-8') as f:
              w=csv.DictWriter(f,fieldnames=['video_id','url','title','source','char_len'])
              w.writeheader()
              w.writerows(sorted(entries, key=lambda r:(r['title'].lower(), r['video_id'])))
          PY

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: transcripts
          path: |
            transcripts/*.vtt
            transcripts/*.txt
            transcripts.csv
          if-no-files-found: warn

      - name: Commit results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          [ -e transcripts.csv ] && git add transcripts.csv || true
          [ -d transcripts ] && git add transcripts/* || true
          git status
          git commit -m "Update transcripts" || echo "No changes"
          git push
